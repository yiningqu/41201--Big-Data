doc_word<-read.table("WordFreqFinal.csv",header=F)
# Create a sparse matrix
library(gamlr)
spm<-sparseMatrix(
i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:ndays,words=words))
dim(spm)
# We select only words at occur at least 5 times
cols<-apply(spm,2,sum)
index<-apply(spm,2,sum)>5
spm<-spm[,index]
# and words that do not occur every day
index<-apply(spm,2,sum)<ndays
spm<-spm[,index]
dim(spm) # we end up with 3183 words
#  *** FDR *** analysis
spm<-spm[-ndays,]
time<-dj[-ndays,1]
time <- as.Date(time, format = "%Y-%m-%d")
# Take returns Rt
par(mfrow=c(1,2))
R<-(dj[-ndays,7]-dj[-1,7])/dj[-1,7]
plot(R~time,type="l")
title(main = "Time Series of Returns")
# Take the log of the maximal spread
V<-log(dj[-ndays,3]-dj[-ndays,4])
plot(V~time,type="l")
title(main = "Time Series of Volatility")
# FDR: we want to pick a few words that correlate with the outcomes (returns and volatility)
# create a dense matrix of word presence
P <- as.data.frame(as.matrix(spm>0))
# we will practice parallel computing now
library(parallel)
margreg <- function(x){
fit <- lm(Outcome~x)
sf <- summary(fit)
return(sf$coef[2,4])
}
cl <- makeCluster(detectCores())
# pull out stars and export to cores
# **** Analysis for Returns ****
Outcome<-R
clusterExport(cl,"Outcome")
# run the regressions in parallel
mrgpvals_R <- unlist(parLapply(cl,P,margreg))
hist(mrgpvals_R,
main="Distribution of P-values for Returns",
xlab="P-Value for R",
ylab="Frequency",
breaks = 70)
# **** Repeat for volatility
Outcome<-V
clusterExport(cl,"Outcome")
# run the regressions in parallel
mrgpvals_V <- unlist(parLapply(cl,P,margreg))
hist(mrgpvals_V,
main="Distribution of P-values for Volatility",
xlab="P-Value for V",
ylab="Frequency",
breaks = 70)
source("fdr.R")
q<-0.1
# Computing the alpha value for 10% FDR
alpha_R <- fdr_cut(mrgpvals_R, q=q)
alpha_V <- fdr_cut(mrgpvals_V, q=q)
# Count the number of significant words at this level
significant_words_R <- sum(mrgpvals_R<=alpha_R)
significant_words_V <- sum(mrgpvals_V<=alpha_V)
cat("alpha value with 10% FDR for R: ", alpha_R, "\n")
cat("alpha value with 10% FDR for V: ", alpha_V, "\n")
cat("Number of significant words for R: ", significant_words_R, "\n")
cat("Number of significant words for V: ", significant_words_V, "\n")
# Identify the 20 smallest p-values for V
p_sorted <- sort(mrgpvals_V)
# Number of tests
n <- length(p_sorted)
# Identify the 20th smallest p-value
p_20 <- p_sorted[20]
# Calculate the FDR value for the 20th smallest p-value
fdr_value_20 <- n * p_20 / 20
expected_false_positives <- fdr_value_20 * 20
cat("FDR value for the 20th smallest p-value:", fdr_value_20, "\n")
cat("Number of discoveries expected to be false: ",expected_false_positives, "\n")
# ***** LASSO analysis *****
# First analyze returns
library(gamlr)
lasso1<- gamlr(spm, y=R, lambda.min.ratio=1e-3)
# Get the summary of the LASSO model
lasso_summary1 <- summary(lasso1)
lambda_opt1 <- lasso1$lambda[which.min(lasso_summary1$aicc)]
cat("Optimal lambda (AICc):", lambda_opt1, "\n")
lambda1_index <- which(lasso1$lambda == lambda_opt1)
# Extract the coefficients corresponding to lambda_opt2
coefficients_opt1 <- lasso1$beta[, lambda1_index]
# Count the number of words selected as predictive of returns R
# (excluding the intercept)
num_words_selected <- sum(coefficients_opt1 != 0)
cat("Number of words selected:", num_words_selected, "\n")
# Extract and print the names of the predictors (words) that have non-zero coefficients
chosen_words <- names(coefficients_opt1)[coefficients_opt1 != 0]
cat("Words chosen by LASSO:\n", paste(chosen_words, collapse = ", "), "\n")
R2_R <- lasso_summary1$r2[lambda1_index]
cat("The in-sample R2 for Returns is: ",R2_R , "\n")
# **** LASSO Analysis of volatility **** #
lasso2<- gamlr(spm, y=V, lambda.min.ratio=1e-3)
# Get the summary of the LASSO model
lasso_summary2 <- summary(lasso2)
lambda_opt2 <- lasso2$lambda[which.min(lasso_summary2$aicc)]
cat("Optimal lambda (AICc):", lambda_opt2, "\n")
lambda2_index <- which(lasso2$lambda == lambda_opt2)
# Extract the coefficients corresponding to lambda_opt2
coefficients_opt2 <- lasso2$beta[, lambda2_index]
# Count the number of words selected as predictive of returns R
# (excluding the previous and intercept)
num_words_selected2 <- sum(coefficients_opt2[-1] != 0)
cat("Number of words selected:", num_words_selected2, "\n")
# Extract and print the names of the predictors (words) that have non-zero coefficients
chosen_words2 <- names(coefficients_opt2)[coefficients_opt2 != 0]
cat("Words chosen by LASSO:\n", paste(chosen_words2, collapse = ", "), "\n")
R2_V <- lasso_summary2$r2[lambda2_index]
cat("The in-sample R2 for Volatility is: ",R2_V , "\n")
# let's try to predict future volatility from past volatility, we will add one more predictor-> volatility from the previous days
Previous<-log(dj[-1,3]-dj[-1,4]) # remove the last return
spm2<-cbind(Previous,spm) # add the previous return to the model matrix
colnames(spm2)[1]<-"previous" # the first column is the previous volatility
lasso3<- gamlr(spm2, y=V, lambda.min.ratio=1e-3)
plot(lasso3)
title(main = "LASSO Path with Volatility on a previous day")
# Get the summary of the LASSO model
lasso_summary3 <- summary(lasso3)
lambda_opt3 <- lasso3$lambda[which.min(lasso_summary3$aicc)]
cat("Optimal lambda (AICc):", lambda_opt3, "\n")
lambda3_index <- which(lasso3$lambda == lambda_opt3)
# Extract the coefficients corresponding to lambda_opt2
coefficients_opt3 <- lasso3$beta[, lambda3_index]
# Count the number of words selected as predictive of returns R
# (excluding the intercept and Volatility)
num_words_selected3 <- sum(coefficients_opt3[-(1)] != 0)
cat("Number of words selected:", num_words_selected3, "\n")
R2_V2 <- lasso_summary3$r2[lambda3_index]
cat("The in-sample R2 for Volatility with Volatility on a previous day is: ",R2_V2 , "\n")
# Find 10 strongest coefficients
effects <- coefficients_opt3
top_10_coef <- names(sort(abs(effects), decreasing = TRUE)[1:10])
print("Top 10 strongest coefficients:")
print(top_10_coef)
coef_terr <- coefficients_opt3["terrorist"]
coef_vt1 <- coefficients_opt3["previous"]
cat("The coefficient for the word 'terrorist' is", coef_terr, "\n")
cat("The coefficient for Vt-1 is", coef_vt1, "\n")
# Bootstrap to obtain s.e. of 1.s.e. chosen lambda
# We apply bootstrap to approximate
# the sampling distribution of lambda
# selected by AICc
# export the data to the clusters
Outcome<-V
clusterExport(cl,"spm2")
clusterExport(cl,"V")
# run 100 bootstrap resample fits
boot_function <- function(ib){
require(gamlr)
fit <- gamlr(spm2[ib,],y=V[ib], lambda.min.ratio=1e-3)
fit$lambda[which.min(AICc(fit))]
}
boots <- 100
n <- nrow(spm2)
resamp <- as.data.frame(
matrix(sample(1:n,boots*n,replace=TRUE),
ncol=boots))
lambda_samp <- unlist(parLapply(cl,resamp,boot_function))
set.seed(41201)
# Sequential bootstrap resampling using a for loop
for (i in 1:boots) {
# Sample indices with replacement
ib <- sample(1:n, n, replace = TRUE)
# Call the bootstrap function and store the result
lambda_samp[i] <- boot_function(ib)
}
# Calculate the standard error of the lambda estimates
lambda_se <- sd(lambda_samp)
cat("Standard error of the selected lambda:", lambda_se, "\n")
# Number of bootstrap samples
n <- length(lambda_samp)
cat("95% Confidence Interval for lambda:",quantile(lambda_samp,0.025), quantile(lambda_samp,0.925), "\n")
# High-dimensional Covariate Adjustment
d <- Previous # this is the treatment
# marginal effect of past on present volatility
summary(glm(V~d))
# we want to isolate the effect of d from external influences. We saw that words can explain some of the volatility.
# Stage 1 LASSO: fit a model for d on x
treat <- gamlr(spm,d,lambda.min.ratio=1e-4)
## Predict d based on the model "treat" and x
dhat <- predict(treat, spm, type="response")
## IS R^2
R2 <- cor(drop(dhat),d)^2
cat("The In-Sample R-squared value is", R2,".\n")
# Stage 2 LASSO: fit a model for V using d, dhat and x
causal <- gamlr(cbind(d,dhat,spm),V,free=2,lmr=1e-4)
## Coefficient of treatment
causald <- coef(causal)["d",]
cat("The effect of d on Volatility is",causald, ".\n")
## naive lasso
naive <- gamlr(cbind(d,spm),V)
## Coefficient for 'd
naived <- coef(naive)["d",]
cat("The effect of d on volatility from a straight (naive) lasso is",naived, "\n")
summary(glm(R~V))
R_prev <- R[-1]  # generate yesterday's return
R_r <- R[-length(R)] # remove the last row to match the length
d_r <- R_prev # this is the treatment
# marginal effect of past on present return
summary(glm(R_r~d_r))
# remove yesterday vol's last row to match the length
Previous_r <- Previous[-length(Previous)]
# Convert the vector Previous_r to a matrix with one column
spm3 <- matrix(Previous_r, ncol = 1)
# Stage 1 LASSO: fit a model for d on x
model_r <- gamlr(spm3,d_r,lambda.min.ratio=1e-4)
dhat_r <- predict(model_r, newdata = spm3, type = "response")
R2_r <- cor(drop(dhat_r), d_r)^2
cat("The In-Sample R-squared value is", R2_r,".\n")
# Stage 2 LASSO: fit a model for R using d, dhat and x
causal_r <- gamlr(cbind(d_r,dhat_r,spm3), R_r, family='gaussian',free=2) ## Coefficient of treatment
causald_r <- coef(causal_r)["d_r",]
cat("The effect of d on today's return is",causald_r, ".\n")
naive_r <- gamlr(cbind(d_r,spm3), R_r , family='gaussian')
## Coefficient for 'd'
naived_r <- coef(naive_r)["d_r",]
cat("The effect of d on today's return from a naive lasso is", naived_r, "\n")
knitr::opts_chunk$set(echo = TRUE)
# Calculate the standard error of the lambda estimates
lambda_se <- sd(lambda_samp)
cat("Standard error of the selected lambda:", lambda_se, "\n")
# Number of bootstrap samples
n <- length(lambda_samp)
cat("95% Confidence Interval for lambda:",quantile(lambda_samp,0.025), quantile(lambda_samp,0.925), "\n")
# Calculate the standard error of the lambda estimates
lambda_se <- sd(lambda_samp)
# Number of bootstrap samples
n <- length(lambda_samp)
# Calculate the mean of the lambda samples
mean_lambda <- mean(lambda_samp)
# Find the t-distribution critical value for 95% confidence interval # Degrees of freedom = n - 1
t_critical <- qt(c(0.025, 0.975), df = n - 1)
# Calculate the 95% confidence interval for lambda using the t-distribution
ci_lambda <- mean_lambda + t_critical * std_error_lambda
# Calculate the standard error of the lambda estimates
lambda_se <- sd(lambda_samp)
# Number of bootstrap samples
n <- length(lambda_samp)
# Calculate the mean of the lambda samples
mean_lambda <- mean(lambda_samp)
# Find the t-distribution critical value for 95% confidence interval # Degrees of freedom = n - 1
t_critical <- qt(c(0.025, 0.975), df = n - 1)
# Calculate the 95% confidence interval for lambda using the t-distribution
ci_lambda <- mean_lambda + t_critical * lambda_se
cat("95% Confidence Interval for lambda:", ci_lambda, "\n")
knitr::opts_chunk$set(echo = TRUE)
library(readtext)
library(SnowballC)
library(tidytext)
data<-readtext("RedditNews.csv",skip=1)
date<-data[2] # this is the day of the news
subset<-date=="7/1/16" # let's take a look at news headlines on 7/1/16
data[subset,3] # we have 23 news headlines
# Read the DJIA data
dj<-read.csv("DJIA.csv")
head(dj) # Open price, highest, lowest and close price
ndays<-nrow(dj) # 1989 days
# Read the words
words<-read.csv("WordsFinal.csv",header=F)
words<-words[,1]
head(words)
# Read the word-day pairings
doc_word<-read.table("WordFreqFinal.csv",header=F)
# Create a sparse matrix
library(gamlr)
spm<-sparseMatrix(
i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:ndays,words=words))
dim(spm)
# We select only words at occur at least 5 times
cols<-apply(spm,2,sum)
index<-apply(spm,2,sum)>5
spm<-spm[,index]
# and words that do not occur every day
index<-apply(spm,2,sum)<ndays
spm<-spm[,index]
dim(spm) # we end up with 3183 words
#  *** FDR *** analysis
spm<-spm[-ndays,]
time<-dj[-ndays,1]
time <- as.Date(time, format = "%Y-%m-%d")
# Take returns Rt
par(mfrow=c(1,2))
R<-(dj[-ndays,7]-dj[-1,7])/dj[-1,7]
plot(R~time,type="l")
title(main = "Time Series of Returns")
# Take the log of the maximal spread
V<-log(dj[-ndays,3]-dj[-ndays,4])
plot(V~time,type="l")
title(main = "Time Series of Volatility")
# FDR: we want to pick a few words that correlate with the outcomes (returns and volatility)
# create a dense matrix of word presence
P <- as.data.frame(as.matrix(spm>0))
# we will practice parallel computing now
library(parallel)
margreg <- function(x){
fit <- lm(Outcome~x)
sf <- summary(fit)
return(sf$coef[2,4])
}
cl <- makeCluster(detectCores())
# pull out stars and export to cores
# **** Analysis for Returns ****
Outcome<-R
clusterExport(cl,"Outcome")
# run the regressions in parallel
mrgpvals_R <- unlist(parLapply(cl,P,margreg))
hist(mrgpvals_R,
main="Distribution of P-values for Returns",
xlab="P-Value for R",
ylab="Frequency",
breaks = 70)
# **** Repeat for volatility
Outcome<-V
clusterExport(cl,"Outcome")
# run the regressions in parallel
mrgpvals_V <- unlist(parLapply(cl,P,margreg))
hist(mrgpvals_V,
main="Distribution of P-values for Volatility",
xlab="P-Value for V",
ylab="Frequency",
breaks = 70)
source("fdr.R")
q<-0.1
# Computing the alpha value for 10% FDR
alpha_R <- fdr_cut(mrgpvals_R, q=q)
alpha_V <- fdr_cut(mrgpvals_V, q=q)
# Count the number of significant words at this level
significant_words_R <- sum(mrgpvals_R<=alpha_R)
significant_words_V <- sum(mrgpvals_V<=alpha_V)
cat("alpha value with 10% FDR for R: ", alpha_R, "\n")
cat("alpha value with 10% FDR for V: ", alpha_V, "\n")
cat("Number of significant words for R: ", significant_words_R, "\n")
cat("Number of significant words for V: ", significant_words_V, "\n")
# Identify the 20 smallest p-values for V
p_sorted <- sort(mrgpvals_V)
# Number of tests
n <- length(p_sorted)
# Identify the 20th smallest p-value
p_20 <- p_sorted[20]
# Calculate the FDR value for the 20th smallest p-value
fdr_value_20 <- n * p_20 / 20
expected_false_positives <- fdr_value_20 * 20
cat("FDR value for the 20th smallest p-value:", fdr_value_20, "\n")
cat("Number of discoveries expected to be false: ",expected_false_positives, "\n")
# ***** LASSO analysis *****
# First analyze returns
library(gamlr)
lasso1<- gamlr(spm, y=R, lambda.min.ratio=1e-3)
# Get the summary of the LASSO model
lasso_summary1 <- summary(lasso1)
lambda_opt1 <- lasso1$lambda[which.min(lasso_summary1$aicc)]
cat("Optimal lambda (AICc):", lambda_opt1, "\n")
lambda1_index <- which(lasso1$lambda == lambda_opt1)
# Extract the coefficients corresponding to lambda_opt2
coefficients_opt1 <- lasso1$beta[, lambda1_index]
# Count the number of words selected as predictive of returns R
# (excluding the intercept)
num_words_selected <- sum(coefficients_opt1 != 0)
cat("Number of words selected:", num_words_selected, "\n")
# Extract and print the names of the predictors (words) that have non-zero coefficients
chosen_words <- names(coefficients_opt1)[coefficients_opt1 != 0]
cat("Words chosen by LASSO:\n", paste(chosen_words, collapse = ", "), "\n")
R2_R <- lasso_summary1$r2[lambda1_index]
cat("The in-sample R2 for Returns is: ",R2_R , "\n")
# **** LASSO Analysis of volatility **** #
lasso2<- gamlr(spm, y=V, lambda.min.ratio=1e-3)
# Get the summary of the LASSO model
lasso_summary2 <- summary(lasso2)
lambda_opt2 <- lasso2$lambda[which.min(lasso_summary2$aicc)]
cat("Optimal lambda (AICc):", lambda_opt2, "\n")
lambda2_index <- which(lasso2$lambda == lambda_opt2)
# Extract the coefficients corresponding to lambda_opt2
coefficients_opt2 <- lasso2$beta[, lambda2_index]
# Count the number of words selected as predictive of returns R
# (excluding the previous and intercept)
num_words_selected2 <- sum(coefficients_opt2[-1] != 0)
cat("Number of words selected:", num_words_selected2, "\n")
# Extract and print the names of the predictors (words) that have non-zero coefficients
chosen_words2 <- names(coefficients_opt2)[coefficients_opt2 != 0]
cat("Words chosen by LASSO:\n", paste(chosen_words2, collapse = ", "), "\n")
R2_V <- lasso_summary2$r2[lambda2_index]
cat("The in-sample R2 for Volatility is: ",R2_V , "\n")
# let's try to predict future volatility from past volatility, we will add one more predictor-> volatility from the previous days
Previous<-log(dj[-1,3]-dj[-1,4]) # remove the last return
spm2<-cbind(Previous,spm) # add the previous return to the model matrix
colnames(spm2)[1]<-"previous" # the first column is the previous volatility
lasso3<- gamlr(spm2, y=V, lambda.min.ratio=1e-3)
plot(lasso3)
title(main = "LASSO Path with Volatility on a previous day")
# Get the summary of the LASSO model
lasso_summary3 <- summary(lasso3)
lambda_opt3 <- lasso3$lambda[which.min(lasso_summary3$aicc)]
cat("Optimal lambda (AICc):", lambda_opt3, "\n")
lambda3_index <- which(lasso3$lambda == lambda_opt3)
# Extract the coefficients corresponding to lambda_opt2
coefficients_opt3 <- lasso3$beta[, lambda3_index]
# Count the number of words selected as predictive of returns R
# (excluding the intercept and Volatility)
num_words_selected3 <- sum(coefficients_opt3[-(1)] != 0)
cat("Number of words selected:", num_words_selected3, "\n")
R2_V2 <- lasso_summary3$r2[lambda3_index]
cat("The in-sample R2 for Volatility with Volatility on a previous day is: ",R2_V2 , "\n")
# Find 10 strongest coefficients
effects <- coefficients_opt3
top_10_coef <- names(sort(abs(effects), decreasing = TRUE)[1:10])
print("Top 10 strongest coefficients:")
print(top_10_coef)
coef_terr <- coefficients_opt3["terrorist"]
coef_vt1 <- coefficients_opt3["previous"]
cat("The coefficient for the word 'terrorist' is", coef_terr, "\n")
cat("The coefficient for Vt-1 is", coef_vt1, "\n")
# Bootstrap to obtain s.e. of 1.s.e. chosen lambda
# We apply bootstrap to approximate
# the sampling distribution of lambda
# selected by AICc
# export the data to the clusters
Outcome<-V
clusterExport(cl,"spm2")
clusterExport(cl,"V")
# run 100 bootstrap resample fits
boot_function <- function(ib){
require(gamlr)
fit <- gamlr(spm2[ib,],y=V[ib], lambda.min.ratio=1e-3)
fit$lambda[which.min(AICc(fit))]
}
boots <- 100
n <- nrow(spm2)
resamp <- as.data.frame(
matrix(sample(1:n,boots*n,replace=TRUE),
ncol=boots))
lambda_samp <- unlist(parLapply(cl,resamp,boot_function))
set.seed(41201)
# Sequential bootstrap resampling using a for loop
for (i in 1:boots) {
# Sample indices with replacement
ib <- sample(1:n, n, replace = TRUE)
# Call the bootstrap function and store the result
lambda_samp[i] <- boot_function(ib)
}
# Calculate the standard error of the lambda estimates
lambda_se <- sd(lambda_samp)
# Number of bootstrap samples
n <- length(lambda_samp)
# Calculate the mean of the lambda samples
mean_lambda <- mean(lambda_samp)
# Find the t-distribution critical value for 95% confidence interval # Degrees of freedom = n - 1
t_critical <- qt(c(0.025, 0.975), df = n - 1)
# Calculate the 95% confidence interval for lambda using the t-distribution
ci_lambda <- mean_lambda + t_critical * lambda_se
cat("95% Confidence Interval for lambda:", ci_lambda, "\n")
# High-dimensional Covariate Adjustment
d <- Previous # this is the treatment
# marginal effect of past on present volatility
summary(glm(V~d))
# we want to isolate the effect of d from external influences. We saw that words can explain some of the volatility.
# Stage 1 LASSO: fit a model for d on x
treat <- gamlr(spm,d,lambda.min.ratio=1e-4)
## Predict d based on the model "treat" and x
dhat <- predict(treat, spm, type="response")
## IS R^2
R2 <- cor(drop(dhat),d)^2
cat("The In-Sample R-squared value is", R2,".\n")
# Stage 2 LASSO: fit a model for V using d, dhat and x
causal <- gamlr(cbind(d,dhat,spm),V,free=2,lmr=1e-4)
## Coefficient of treatment
causald <- coef(causal)["d",]
cat("The effect of d on Volatility is",causald, ".\n")
## naive lasso
naive <- gamlr(cbind(d,spm),V)
## Coefficient for 'd
naived <- coef(naive)["d",]
cat("The effect of d on volatility from a straight (naive) lasso is",naived, "\n")
summary(glm(R~V))
R_prev <- R[-1]  # generate yesterday's return
R_r <- R[-length(R)] # remove the last row to match the length
d_r <- R_prev # this is the treatment
# marginal effect of past on present return
summary(glm(R_r~d_r))
# remove yesterday vol's last row to match the length
Previous_r <- Previous[-length(Previous)]
# Convert the vector Previous_r to a matrix with one column
spm3 <- matrix(Previous_r, ncol = 1)
# Stage 1 LASSO: fit a model for d on x
model_r <- gamlr(spm3,d_r,lambda.min.ratio=1e-4)
dhat_r <- predict(model_r, newdata = spm3, type = "response")
R2_r <- cor(drop(dhat_r), d_r)^2
cat("The In-Sample R-squared value is", R2_r,".\n")
# Stage 2 LASSO: fit a model for R using d, dhat and x
causal_r <- gamlr(cbind(d_r,dhat_r,spm3), R_r, family='gaussian',free=2) ## Coefficient of treatment
causald_r <- coef(causal_r)["d_r",]
cat("The effect of d on today's return is",causald_r, ".\n")
naive_r <- gamlr(cbind(d_r,spm3), R_r , family='gaussian')
## Coefficient for 'd'
naived_r <- coef(naive_r)["d_r",]
cat("The effect of d on today's return from a naive lasso is", naived_r, "\n")
