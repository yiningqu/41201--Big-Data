# ***** AMAZON REVIEWS 

# READ REVIEWS

data<-read.table("Review_subset.csv",header=TRUE)
dim(data)

# 13319 reviews
# ProductID: Amazon ASIN product code
# UserID:  id of the reviewer
# Score: numeric from 1 to 5
# Time: date of the review
# Summary: text review
# nrev: number of reviews by this user
# Length: length of the review (number of words)

# READ WORDS

words<-read.table("words.csv")
words<-words[,1]
length(words)
#1125 unique words

# READ text-word pairings file

doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
# Review ID: row of the file  Review_subset
# Word ID: index of the word
# Times Word: number of times this word occurred in the text


# We'll do 1125 univariate regressions of 
# star rating on word presence, one for each word.
# Each regression will return a p-value, and we can
# use this as an initial screen for useful words.

# Don't worry if you do not understand the code now.
# We will go over similar code in  the class in a few weeks.

# Create a sparse matrix of word presence


library(gamlr)

spm<-sparseMatrix(i=doc_word[,1],
                  j=doc_word[,2],
                  x=doc_word[,3],
                  dimnames=list(id=1:nrow(data),words=words))

dim(spm)
# 13319 reviews using 1125 words

# Create a dense matrix of word presence

P <- as.data.frame(as.matrix(spm>0))

library(parallel)

margreg <- function(p){
	fit <- lm(stars~p)
	sf <- summary(fit)
	return(sf$coef[2,4]) 
}

# The code below is an example of parallel computing
# No need to understand details now, we will discuss more later

cl <- makeCluster(detectCores())

# Pull out stars and export to cores

stars <- data$Score

clusterExport(cl,"stars") 

# Run the regressions in parallel

mrgpvals <- unlist(parLapply(cl,P,margreg))

# If parallel stuff is not working, 
# you can also just do (in serial):
# mrgpvals <- c()
# for(j in 1:1125){
# 	print(j)
# 	mrgpvals <- c(mrgpvals,margreg(P[,j]))
# }
# make sure we have names

names(mrgpvals) <- colnames(P)

# The p-values are stored in mrgpvals 



hist(mrgpvals, 
     main="Histogram of P-Values",   
     xlab="P-Value",                 
     ylab="Frequency")   

# Count the number of tests significant at alpha level 0.05
sum(mrgpvals < 0.05)

# Count the number of tests significant at alpha level 0.01
sum(mrgpvals < 0.01)


# Sort p-values in ascending order
p_sorted <- sort(mrgpvals)

# Number of tests
n <- length(p_sorted)

# Desired FDR level
FDR <- 0.01

# Calculate p-value cut-off
bh_critical_values <- (1:n) / n * FDR

# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < bh_critical_values))
fdr_cutoff <- p_sorted[cutoff_index]
print(paste("FDR 1% cutoff p-value:", fdr_cutoff))


# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "Critical values", "FDR cutoff"),
       col = c("black", "red", "blue"), lty = c(1, 1, 2))


num_reject <- sum(p_sorted <= fdr_cutoff)
num_reject * 0.01

p_sorted[1:10]

