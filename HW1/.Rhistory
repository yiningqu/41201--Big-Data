data<-read.table("Review_subset.csv",header=TRUE)
setwd("/Users/yiningqu/Desktop/研究生/Big data/HW1")
# ***** AMAZON REVIEWS
# READ REVIEWS
data<-read.table("Review_subset.csv",header=TRUE)
dim(data)
# 13319 reviews
# ProductID: Amazon ASIN product code
# UserID:  id of the reviewer
# Score: numeric from 1 to 5
# Time: date of the review
# Summary: text review
# nrev: number of reviews by this user
# Length: length of the review (number of words)
# READ WORDS
words<-read.table("words.csv")
words<-words[,1]
length(words)
#1125 unique words
# READ text-word pairings file
doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
# Review ID: row of the file  Review_subset
# Word ID: index of the word
# Times Word: number of times this word occurred in the text
# We'll do 1125 univariate regressions of
# star rating on word presence, one for each word.
# Each regression will return a p-value, and we can
# use this as an initial screen for useful words.
# Don't worry if you do not understand the code now.
# We will go over similar code in  the class in a few weeks.
# Create a sparse matrix of word presence
library(gamlr)
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),words=words))
dim(spm)
# 13319 reviews using 1125 words
# Create a dense matrix of word presence
P <- as.data.frame(as.matrix(spm>0))
library(parallel)
margreg <- function(p){
fit <- lm(stars~p)
sf <- summary(fit)
return(sf$coef[2,4])
}
# The code below is an example of parallel computing
# No need to understand details now, we will discuss more later
cl <- makeCluster(detectCores())
# Pull out stars and export to cores
stars <- data$Score
clusterExport(cl,"stars")
# Run the regressions in parallel
mrgpvals <- unlist(parLapply(cl,P,margreg))
# If parallel stuff is not working,
# you can also just do (in serial):
# mrgpvals <- c()
# for(j in 1:1125){
# 	print(j)
# 	mrgpvals <- c(mrgpvals,margreg(P[,j]))
# }
# make sure we have names
names(mrgpvals) <- colnames(P)
# The p-values are stored in mrgpvals
hist(mrgpvals)
hist(mrgpvals,
main="Histogram of P-Values",
xlab="P-Value",
ylab="Frequency",
col="blue")
hist(mrgpvals,
main="Histogram of P-Values",
xlab="P-Value",
ylab="Frequency")
hist(mrgpvals,
main="Histogram of P-Values",
xlab="P-Value",
ylab="Frequency")
# Count the number of tests significant at alpha level 0.05
significant_at_05 <- sum(mrgpvals < 0.05)
# Count the number of tests significant at alpha level 0.01
significant_at_01 <- sum(mrgpvals < 0.01)
# Count the number of tests significant at alpha level 0.05
sum(mrgpvals < 0.05)
# Count the number of tests significant at alpha level 0.01
sum(mrgpvals < 0.01)
hist(mrgpvals,
main="Histogram of P-Values",
xlab="P-Value",
ylab="Frequency")
# Count the number of tests significant at alpha level 0.05
sum(mrgpvals < 0.05)
# Count the number of tests significant at alpha level 0.01
sum(mrgpvals < 0.01)
# Sort p-values in ascending order
p_sorted <- sort(mrgpvals)
# Number of tests
n <- length(p_sorted)
# Desired FDR level
FDR <- 0.01
# Calculate p-value cut-off
cutoff <- (1:n) / n * FDR
# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < cutoff))
p_sorted[cutoff_index]
# Sort p-values in ascending order
p_sorted <- sort(mrgpvals)
# Number of tests
n <- length(p_sorted)
# Desired FDR level
FDR <- 0.01
# Calculate p-value cut-off
cutoff <- (1:n) / n * FDR
# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < cutoff))
fdr_cutoff <- p_sorted[cutoff_index]
print(paste("FDR 1% cutoff p-value:", fdr_cutoff))
plot(p_sorted, type = 'h', lwd = 2, main = "P-Values and FDR Cutoff",
xlab = "Rank", ylab = "P-Value")
plot(p_sorted)
plot(p_sorted, type = 'h', lwd = 2, main = "P-Values and FDR Cutoff",
xlab = "k", ylab = "orderedd p-values")
plot(p_sorted, type = 'h',
xlab = "k", ylab = "orderedd p-values")
plot(p_sorted, type = 'h', lwd = 2,
xlab = "k", ylab = "orderedd p-values")
plot(p_sorted, type = 'h', lwd = 2, main = "P-Values and FDR Cutoff",
xlab = "k", ylab = "orderedd p-values")
plot(p_sorted, type = 'h', lwd = 2, main = "P-Values and FDR Cutoff",
xlab = "Rank", ylab = "P-Value", ylim = c(0, max(bh_critical_values)))
# Calculate p-value cut-off
bh_critical_values <- (1:n) / n * FDR
# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < bh_critical_values))
fdr_cutoff <- p_sorted[cutoff_index]
print(paste("FDR 1% cutoff p-value:", fdr_cutoff))
plot(p_sorted, type = 'h', lwd = 2, main = "P-Values and FDR Cutoff",
xlab = "Rank", ylab = "P-Value", ylim = c(0, max(bh_critical_values)))
# Assuming mrgpvals is your vector of p-values and the previous steps have been executed
library(ggplot2)
# Create a dataframe for plotting
plot_data <- data.frame(
Rank = 1:n,
P_Value = p_sorted,
BH_Critical_Value = bh_critical_values
)
# Create the plot with ggplot2
p <- ggplot(plot_data, aes(x = Rank, y = P_Value)) +
geom_point() +  # Add points
geom_line(aes(y = BH_Critical_Value), color = "red") +  # Add BH critical value line
geom_point(data = subset(plot_data, P_Value < fdr_cutoff), color = "blue") +  # Highlight points below the cutoff
theme_minimal() +
labs(title = "P-Values and BH FDR Cutoff",
x = "Rank",
y = "P-Value") +
geom_hline(yintercept = fdr_cutoff, linetype = "dashed", color = "blue")  # Add horizontal line for FDR cutoff
# Assuming mrgpvals is your vector of p-values and the previous steps have been executed
library(ggplot2)
install.packages("ggplot2")
# Assuming mrgpvals is your vector of p-values and the previous steps have been executed
library(ggplot2)
# Create a dataframe for plotting
plot_data <- data.frame(
Rank = 1:n,
P_Value = p_sorted,
BH_Critical_Value = bh_critical_values
)
# Create the plot with ggplot2
p <- ggplot(plot_data, aes(x = Rank, y = P_Value)) +
geom_point() +  # Add points
geom_line(aes(y = BH_Critical_Value), color = "red") +  # Add BH critical value line
geom_point(data = subset(plot_data, P_Value < fdr_cutoff), color = "blue") +  # Highlight points below the cutoff
theme_minimal() +
labs(title = "P-Values and BH FDR Cutoff",
x = "Rank",
y = "P-Value") +
geom_hline(yintercept = fdr_cutoff, linetype = "dashed", color = "blue")  # Add horizontal line for FDR cutoff
# Print the plot
print(p)
# Plot the sorted p-values
plot(sorted_pvals, type = "l", xlab = "Rank", ylab = "P-value", main = "P-values and FDR C
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "P-value", main = "P-values and FDR Cutoff")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "BH critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "BH critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
lines(fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "BH critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "BH critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "Critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
## extract p-value cutoff for E[fdf] < q
fdr_cut <- function(pvals, q, plotit=FALSE, ...){
pvals <- pvals[!is.na(pvals)]
N <- length(pvals)
k <- rank(pvals, ties.method="min")
alpha <- max(pvals[ pvals<= (q*k/N) ])
if(plotit){
sig <- factor(pvals<=alpha)
o <- order(pvals)
plot(pvals[o], col=c("grey60","red")[sig[o]], pch=20, ...,
ylab="p-values", xlab="tests ordered by p-value", main = paste('FDR =',q))
lines(1:N, q*(1:N)/N)
}
return(alpha)
}
# Simulation
p<-100
n<-200
n_signals<-20
q<-0.2
X<-matrix(rnorm(n*p),n,p)
# Scenario 1
# 20 signals and 80 noise coefficients
beta<-c(rep(3,n_signals),rep(0,p-n_signals))
# Scenario 2
# 100 noise coefficients
beta<-c(rep(0,n_signals),rep(0,p-n_signals))
Y<-X%*%beta+rnorm(n,0,1)
pvals<-numeric(p)
for(i in (1:p)){
model<-glm(Y~X[,i])
pvals[i]<-summary(model)$coefficients[2,4]
}
hist(pvals,col="lightblue",breaks=10)
pvals_ordered<-pvals[order(pvals,decreasing=F)]
plot(pvals_ordered,pch=19)
abline(0,1/p)
source("fdr.R")
cutoff <- fdr_cut(pvals, q)
cutoff
abline(h=cutoff,lty=2,col=3,lwd=3)
abline(0,q/p,col=2,lwd=2)
signif <- pvals_ordered <= cutoff
points(pvals_ordered,
col=signif+1,pch=19) # The red dots are discoveries
table(pvals<=cutoff) # number of discoveries and non-discoveries
(1:p)[pvals<=cutoff]
t<-table(beta,pvals<=cutoff)
t
# what is FDP (false discovery proportion)
t[1,2]/(sum(t[,2]))
# ***** AMAZON REVIEWS
# READ REVIEWS
data<-read.table("Review_subset.csv",header=TRUE)
dim(data)
# 13319 reviews
# ProductID: Amazon ASIN product code
# UserID:  id of the reviewer
# Score: numeric from 1 to 5
# Time: date of the review
# Summary: text review
# nrev: number of reviews by this user
# Length: length of the review (number of words)
# READ WORDS
words<-read.table("words.csv")
words<-words[,1]
length(words)
#1125 unique words
# READ text-word pairings file
doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
# Review ID: row of the file  Review_subset
# Word ID: index of the word
# Times Word: number of times this word occurred in the text
# We'll do 1125 univariate regressions of
# star rating on word presence, one for each word.
# Each regression will return a p-value, and we can
# use this as an initial screen for useful words.
# Don't worry if you do not understand the code now.
# We will go over similar code in  the class in a few weeks.
# Create a sparse matrix of word presence
library(gamlr)
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),words=words))
dim(spm)
# 13319 reviews using 1125 words
# Create a dense matrix of word presence
P <- as.data.frame(as.matrix(spm>0))
library(parallel)
margreg <- function(p){
fit <- lm(stars~p)
sf <- summary(fit)
return(sf$coef[2,4])
}
# The code below is an example of parallel computing
# No need to understand details now, we will discuss more later
cl <- makeCluster(detectCores())
# Pull out stars and export to cores
stars <- data$Score
clusterExport(cl,"stars")
# Run the regressions in parallel
mrgpvals <- unlist(parLapply(cl,P,margreg))
# If parallel stuff is not working,
# you can also just do (in serial):
# mrgpvals <- c()
# for(j in 1:1125){
# 	print(j)
# 	mrgpvals <- c(mrgpvals,margreg(P[,j]))
# }
# make sure we have names
names(mrgpvals) <- colnames(P)
# The p-values are stored in mrgpvals
hist(mrgpvals,
main="Histogram of P-Values",
xlab="P-Value",
ylab="Frequency")
# Count the number of tests significant at alpha level 0.05
sum(mrgpvals < 0.05)
# Count the number of tests significant at alpha level 0.01
sum(mrgpvals < 0.01)
# Sort p-values in ascending order
p_sorted <- sort(mrgpvals)
# Number of tests
n <- length(p_sorted)
# Desired FDR level
FDR <- 0.01
# Calculate p-value cut-off
bh_critical_values <- (1:n) / n * FDR
# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < bh_critical_values))
fdr_cutoff <- p_sorted[cutoff_index]
print(paste("FDR 1% cutoff p-value:", fdr_cutoff))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "Critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
sum(p_sorted <= fdr_cutoff)
num_reject <- sum(p_sorted <= fdr_cutoff)
num_reject * 0.01
names(sort(mrgpvals))[1:10]
names(p_sorted)[1:10]
p_sorted[1:10]
knitr::opts_chunk$set(echo = TRUE)
# ***** AMAZON REVIEWS
# READ REVIEWS
data<-read.table("Review_subset.csv",header=TRUE)
dim(data)
# 13319 reviews
# ProductID: Amazon ASIN product code
# UserID:  id of the reviewer
# Score: numeric from 1 to 5
# Time: date of the review
# Summary: text review
# nrev: number of reviews by this user
# Length: length of the review (number of words)
# READ WORDS
words<-read.table("words.csv")
words<-words[,1]
length(words)
#1125 unique words
# READ text-word pairings file
doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
# Review ID: row of the file  Review_subset
# Word ID: index of the word
# Times Word: number of times this word occurred in the text
# We'll do 1125 univariate regressions of
# star rating on word presence, one for each word.
# Each regression will return a p-value, and we can
# use this as an initial screen for useful words.
# Don't worry if you do not understand the code now.
# We will go over similar code in  the class in a few weeks.
# Create a sparse matrix of word presence
library(gamlr)
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),words=words))
dim(spm)
# 13319 reviews using 1125 words
# Create a dense matrix of word presence
P <- as.data.frame(as.matrix(spm>0))
library(parallel)
margreg <- function(p){
fit <- lm(stars~p)
sf <- summary(fit)
return(sf$coef[2,4])
}
# The code below is an example of parallel computing
# No need to understand details now, we will discuss more later
cl <- makeCluster(detectCores())
# Pull out stars and export to cores
stars <- data$Score
clusterExport(cl,"stars")
# Run the regressions in parallel
mrgpvals <- unlist(parLapply(cl,P,margreg))
# If parallel stuff is not working,
# you can also just do (in serial):
# mrgpvals <- c()
# for(j in 1:1125){
# 	print(j)
# 	mrgpvals <- c(mrgpvals,margreg(P[,j]))
# }
# make sure we have names
names(mrgpvals) <- colnames(P)
# The p-values are stored in mrgpvals
hist(mrgpvals,
main="Histogram of P-Values",
xlab="P-Value",
ylab="Frequency")
# Count the number of tests significant at alpha level 0.05
sum(mrgpvals < 0.05)
# Count the number of tests significant at alpha level 0.01
sum(mrgpvals < 0.01)
# Sort p-values in ascending order
p_sorted <- sort(mrgpvals)
# Number of tests
n <- length(p_sorted)
# Desired FDR level
FDR <- 0.01
# Calculate p-value cut-off
bh_critical_values <- (1:n) / n * FDR
# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < bh_critical_values))
fdr_cutoff <- p_sorted[cutoff_index]
print(paste("FDR 1% cutoff p-value:", fdr_cutoff))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "Critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Sort p-values in ascending order
p_sorted <- sort(mrgpvals)
# Number of tests
n <- length(p_sorted)
# Desired FDR level
FDR <- 0.01
# Calculate p-value cut-off
bh_critical_values <- (1:n) / n * FDR
# Find the largest p-value where the p-value is less than the critical value
cutoff_index <- max(which(p_sorted < bh_critical_values))
fdr_cutoff <- p_sorted[cutoff_index]
print(paste("FDR 1% cutoff p-value:", fdr_cutoff))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "Critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Plot the sorted p-values
plot(p_sorted, type = "l", xlab = "Rank", ylab = "Ordered p-value")
# Plot the BH critical values
lines(bh_critical_values, col = "red")
# Add a horizontal line at the FDR cutoff
abline(h = fdr_cutoff, col = "blue", lty = 2)
# Add a legend
legend("topright", legend = c("Sorted p-values", "Critical values", "FDR cutoff"),
col = c("black", "red", "blue"), lty = c(1, 1, 2))
num_reject <- sum(p_sorted <= fdr_cutoff)
num_reject * 0.01
num_reject <- sum(p_sorted <= fdr_cutoff)
print(num_reject)
print(num_reject * 0.01)
p_sorted[1:10]
---
title: "HW1"
##### ******** Mortgage and Home Sales Data ******** #####
## Read in the data
homes <- read.csv("homes2004.csv")
