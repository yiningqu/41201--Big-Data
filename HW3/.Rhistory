knitr::opts_chunk$set(echo = TRUE,
#include = TRUE,
fig.width = 6, fig.height = 4,
results='hide',
warning = FALSE,
cache = TRUE,
digits = 3,
width = 48)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
knitr::opts_chunk$set(echo = TRUE,
#include = TRUE,
fig.width = 6, fig.height = 4,
results='hide',
warning = FALSE,
cache = TRUE,
digits = 3,
width = 48)
library(knitr) # library for nice R markdown output
# READ REVIEWS
data<-read.table("Review_subset.csv",header=TRUE)
dim(data)
# 13319 reviews
# ProductID: Amazon ASIN product code
# UserID:  id of the reviewer
# Score: numeric from 1 to 5
# Time: date of the review
# Summary: text review
# nrev: number of reviews by this user
# Length: length of the review (number of words)
# READ WORDS
words<-read.table("words.csv")
words<-words[,1]
length(words)
#1125 unique words
# READ text-word pairings file
doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
# Review ID: row of the file  Review_subset
# Word ID: index of the word
# Times Word: number of times this word occurred in the text
# Let's define the binary outcome
# Y=1 if the rating was 5 stars
# Y=0 otherwise
Y<-as.numeric(data$Score==5)
# (a) Use only product category as a predictor
library(gamlr)
source("naref.R")
# Cast the product category as a factor
data$Prod_Category<-as.factor(data$Prod_Category)
class(data$Prod_Category)
# Since product category is a factor, we want to relevel it for the LASSO.
# We want each coefficient to be an intercept for each factor level rather than a contrast.
# Check the extra slides at the end of the lecture.
# look inside naref.R. This function relevels the factors for us.
data$Prod_Category<-naref(data$Prod_Category)
# Create a design matrix using only products
products<-data.frame(data$Prod_Category)
x_cat<-sparse.model.matrix(~., data=products)[,-1]
# Sparse matrix, storing 0's as .'s
# Remember that we removed intercept so that each category
# is standalone, not a contrast relative to the baseline category
colnames(x_cat)<-levels(data$Prod_Category)[-1]
# let's call the columns of the sparse design matrix as the product categories
# Let's fit the LASSO with just the product categories
lasso1<- gamlr(x_cat, 	y=Y, standardize=FALSE,family="binomial",
lambda.min.ratio=1e-3)
# The number of observations
n <- nrow(x_cat)
# Calculate the AICc for each model along the LASSO path
aic_values <- sapply(1:length(lasso1$lambda), function(i) {
deviance <- lasso1$deviance[i]
num_params <- lasso1$df[i]  # number of non-zero coefficients
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index <- which.min(aic_values)
D <- lasso1$deviance[optimal_index]
null_model <- glm(Y ~ 1, family = "binomial", data = data.frame(Y = Y))
D0 <- null_model$deviance
R2 <- 1 - (D / D0)
cat("in-sample R-squared for the AICc slice of the LASSO path is ", R2, "\n")
# Fit a LASSO with all 142 product categories and 1125 words
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),
words=words))
dim(spm) # 13319 reviews using 1125 words
x_cat2<-cbind(x_cat,spm)
lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")
n <- nrow(x_cat2)
# Calculate the AICc for each model along the LASSO path of lasso2
aic_values2 <- sapply(1:length(lasso2$lambda), function(i) {
deviance <- lasso2$deviance[i]
num_params <- lasso2$df[i]  # number of non-zero coefficients
# Calculate AICc using the number of observations specific to lasso2
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index2 <- which.min(aic_values2)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
cv.fit <- cv.gamlr(x_cat2,
y=Y,
lambda.min.ratio=1e-3,
family="binomial",
verb=TRUE)
View(optimal_coefs)
# Let's define the binary outcome
# Y=1 if the rating was 5 stars
# Y=0 otherwise
Y<-as.numeric(data$Score==5)
# (a) Use only product category as a predictor
library(gamlr)
source("naref.R")
# Cast the product category as a factor
data$Prod_Category<-as.factor(data$Prod_Category)
class(data$Prod_Category)
# Since product category is a factor, we want to relevel it for the LASSO.
# We want each coefficient to be an intercept for each factor level rather than a contrast.
# Check the extra slides at the end of the lecture.
# look inside naref.R. This function relevels the factors for us.
data$Prod_Category<-naref(data$Prod_Category)
# Create a design matrix using only products
products<-data.frame(data$Prod_Category)
x_cat<-sparse.model.matrix(~., data=products)[,-1]
# Sparse matrix, storing 0's as .'s
# Remember that we removed intercept so that each category
# is standalone, not a contrast relative to the baseline category
colnames(x_cat)<-levels(data$Prod_Category)[-1]
# let's call the columns of the sparse design matrix as the product categories
# Let's fit the LASSO with just the product categories
lasso1<- gamlr(x_cat, 	y=Y, standardize=FALSE,family="binomial",
lambda.min.ratio=1e-3)
# The number of observations
n <- nrow(x_cat)
# Calculate the AICc for each model along the LASSO path
aic_values <- sapply(1:length(lasso1$lambda), function(i) {
deviance <- lasso1$deviance[i]
num_params <- lasso1$df[i]  # number of non-zero coefficients
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index <- which.min(aic_values)
D <- lasso1$deviance[optimal_index]
null_model <- glm(Y ~ 1, family = "binomial", data = data.frame(Y = Y))
D0 <- null_model$deviance
R2 <- 1 - (D / D0)
cat("in-sample R-squared for the AICc slice of the LASSO path is ", R2, "\n")
# The number of observations
n <- nrow(x_cat)
# Calculate the AICc for each model along the LASSO path
aic_values <- sapply(1:length(lasso1$lambda), function(i) {
deviance <- lasso1$deviance[i]
num_params <- lasso1$df[i]  # number of non-zero coefficients
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index <- which.min(aic_values)
D <- lasso1$deviance[optimal_index]
null_model <- glm(Y ~ 1, family = "binomial", data = data.frame(Y = Y))
D0 <- null_model$deviance
R2 <- 1 - (D / D0)
cat("in-sample R-squared for the AICc slice of the LASSO path is ", R2, "\n")
# Fit a LASSO with all 142 product categories and 1125 words
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),
words=words))
dim(spm) # 13319 reviews using 1125 words
x_cat2<-cbind(x_cat,spm)
lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")
n <- nrow(x_cat2)
# Calculate the AICc for each model along the LASSO path of lasso2
aic_values2 <- sapply(1:length(lasso2$lambda), function(i) {
deviance <- lasso2$deviance[i]
num_params <- lasso2$df[i]  # number of non-zero coefficients
# Calculate AICc using the number of observations specific to lasso2
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index2 <- which.min(aic_values2)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
# as.vector(optimal_coefs@x)
optimal_coefs@x
View(optimal_coefs)
View(optimal_coefs)
optimal_coefs$Dimnames
optimal_coefs@x
non_zero_indices <- which(optimal_coefs@x != 0)
variable_names <- rownames(optimal_coefs)[non_zero_indices]
unique_variable_names <- unique(variable_names)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
non_zero_indices <- which(optimal_coefs@x != 0)
variable_names <- rownames(optimal_coefs)[non_zero_indices]
unique_variable_names <- unique(variable_names)
num_words_selected <- sum(optimal_coefs != 0) - (142 + 1)
non_zero_coefs <- optimal_coefs[optimal_coefs@i + 1]
non_zero_coef_names <- rownames(optimal_coefs)[optimal_coefs@i + 1]
num_predictive_words <- sum(non_zero_coefs != 0) - (142 + 1)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
# Identify non-zero coefficients (predictive words)
non_zero_coefs <- optimal_coefs[optimal_coefs@i + 1]  # adjust indices for R's 1-based indexing
# Names of non-zero coefficients (words and product categories)
non_zero_coef_names <- rownames(optimal_coefs)[optimal_coefs@i + 1]  # adjust indices for R's 1-based indexing
# Number of predictive words (excluding product categories and intercept)
num_predictive_words <- sum(non_zero_coefs != 0) - (142 + 1)  # adjust number of categories if necessary
# Print number of predictive words
print(num_predictive_words)
top_words_indices <- order(non_zero_coefs, decreasing = TRUE)[1:10]
top_words <- non_zero_coef_names[top_words_indices]
top_word_effects <- non_zero_coefs[top_words_indices]
# Print top 10 predictive words and their effects
print(data.frame(word = top_words, effect = top_word_effects))
# Fit a LASSO with all 142 product categories and 1125 words
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),
words=words))
dim(spm) # 13319 reviews using 1125 words
x_cat2<-cbind(x_cat,spm)
lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")
n <- nrow(x_cat2)
# Calculate the AICc for each model along the LASSO path of lasso2
aic_values2 <- sapply(1:length(lasso2$lambda), function(i) {
deviance <- lasso2$deviance[i]
num_params <- lasso2$df[i]  # number of non-zero coefficients
# Calculate AICc using the number of observations specific to lasso2
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index2 <- which.min(aic_values2)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
non_zero_indices <- which(optimal_coefs@x != 0)
variable_names <- rownames(optimal_coefs)[non_zero_indices]
unique_variable_names <- unique(variable_names)
num_words_selected <- sum(optimal_coefs != 0) - (142 + 1)
cat("in-sample R-squared for the AICc slice of the LASSO path is ", R2, "\n")
print(variable_names)
# Fit a LASSO with all 142 product categories and 1125 words
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),
words=words))
dim(spm) # 13319 reviews using 1125 words
x_cat2<-cbind(x_cat,spm)
lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")
n <- nrow(x_cat2)
# Calculate the AICc for each model along the LASSO path of lasso2
aic_values2 <- sapply(1:length(lasso2$lambda), function(i) {
deviance <- lasso2$deviance[i]
num_params <- lasso2$df[i]  # number of non-zero coefficients
# Calculate AICc using the number of observations specific to lasso2
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index2 <- which.min(aic_values2)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
non_zero_indices <- which(optimal_coefs@x != 0)
variable_names <- rownames(optimal_coefs)[non_zero_indices]
unique_variable_names <- unique(variable_names)
num_words_selected <- sum(optimal_coefs != 0) - (142 + 1)
cat("The amount of words selected as predictive of a 5 star review is ", num_words_selected, "\n")
knitr::opts_chunk$set(echo = TRUE,
#include = TRUE,
fig.width = 6, fig.height = 4,
results='hide',
warning = FALSE,
cache = TRUE,
digits = 3,
width = 48)
library(knitr) # library for nice R markdown output
# READ REVIEWS
data<-read.table("Review_subset.csv",header=TRUE)
dim(data)
# 13319 reviews
# ProductID: Amazon ASIN product code
# UserID:  id of the reviewer
# Score: numeric from 1 to 5
# Time: date of the review
# Summary: text review
# nrev: number of reviews by this user
# Length: length of the review (number of words)
# READ WORDS
words<-read.table("words.csv")
words<-words[,1]
length(words)
#1125 unique words
# READ text-word pairings file
doc_word<-read.table("word_freq.csv")
names(doc_word)<-c("Review ID","Word ID","Times Word" )
# Review ID: row of the file  Review_subset
# Word ID: index of the word
# Times Word: number of times this word occurred in the text
# Let's define the binary outcome
# Y=1 if the rating was 5 stars
# Y=0 otherwise
Y<-as.numeric(data$Score==5)
# (a) Use only product category as a predictor
library(gamlr)
source("naref.R")
# Cast the product category as a factor
data$Prod_Category<-as.factor(data$Prod_Category)
class(data$Prod_Category)
# Since product category is a factor, we want to relevel it for the LASSO.
# We want each coefficient to be an intercept for each factor level rather than a contrast.
# Check the extra slides at the end of the lecture.
# look inside naref.R. This function relevels the factors for us.
data$Prod_Category<-naref(data$Prod_Category)
# Create a design matrix using only products
products<-data.frame(data$Prod_Category)
x_cat<-sparse.model.matrix(~., data=products)[,-1]
# Sparse matrix, storing 0's as .'s
# Remember that we removed intercept so that each category
# is standalone, not a contrast relative to the baseline category
colnames(x_cat)<-levels(data$Prod_Category)[-1]
# let's call the columns of the sparse design matrix as the product categories
# Let's fit the LASSO with just the product categories
lasso1<- gamlr(x_cat, 	y=Y, standardize=FALSE,family="binomial",
lambda.min.ratio=1e-3)
# The number of observations
n <- nrow(x_cat)
# Calculate the AICc for each model along the LASSO path
aic_values <- sapply(1:length(lasso1$lambda), function(i) {
deviance <- lasso1$deviance[i]
num_params <- lasso1$df[i]  # number of non-zero coefficients
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index <- which.min(aic_values)
D <- lasso1$deviance[optimal_index]
null_model <- glm(Y ~ 1, family = "binomial", data = data.frame(Y = Y))
D0 <- null_model$deviance
R2 <- 1 - (D / D0)
cat("in-sample R-squared for the AICc slice of the LASSO path is ", R2, "\n")
# Fit a LASSO with all 142 product categories and 1125 words
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),
words=words))
dim(spm) # 13319 reviews using 1125 words
x_cat2<-cbind(x_cat,spm)
lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")
n <- nrow(x_cat2)
# Calculate the AICc for each model along the LASSO path of lasso2
aic_values2 <- sapply(1:length(lasso2$lambda), function(i) {
deviance <- lasso2$deviance[i]
num_params <- lasso2$df[i]  # number of non-zero coefficients
# Calculate AICc using the number of observations specific to lasso2
aicc <- deviance + 2 * num_params * (n / (n - num_params - 1))
return(aicc)
})
optimal_index2 <- which.min(aic_values2)
optimal_coefs <- coef(lasso2, s=lasso2$beta[optimal_index2])
non_zero_indices <- which(optimal_coefs@x != 0)
variable_names <- rownames(optimal_coefs)[non_zero_indices]
unique_variable_names <- unique(variable_names)
num_words_selected <- sum(optimal_coefs != 0) - (142 + 1)
cat("The amount of words selected as predictive of a 5 star review is ", num_words_selected, "\n")
cv.fit <- cv.gamlr(x_cat2,
y=Y,
lambda.min.ratio=1e-3,
family="binomial",
verb=TRUE)
optimal_index2 <- which.min(aic_values2)
lasso2_summary = summary(lasso2)
optimal_lambda <- lasso2_summary$lambda[optimal_index]
lasso2_summary = summary(lasso2)
optimal_index2 <- which.min(lasso2_summary$aicc)
optimal_lambda <- lasso2_summary$lambda[optimal_index]
optimal_lambda <- lasso2_summary$lambda[optimal_index2]
num_words_selected <- coef(lasso2) - 143
coef_without <- coef(lasso2) - 143
num_word_selected <- 1125 - sum(coef_without == 0)
# Fit a LASSO with all 142 product categories and 1125 words
spm<-sparseMatrix(i=doc_word[,1],
j=doc_word[,2],
x=doc_word[,3],
dimnames=list(id=1:nrow(data),
words=words))
dim(spm) # 13319 reviews using 1125 words
x_cat2<-cbind(x_cat,spm)
lasso2 <- gamlr(x_cat2, y=Y,lambda.min.ratio=1e-3,family="binomial")
lasso2_summary = summary(lasso2)
optimal_index2 <- which.min(lasso2_summary$aicc)
optimal_lambda <- lasso2_summary$lambda[optimal_index2]
coef_without <- coef(lasso2) - 143
num_word_selected <- 1125 - sum(coef_without == 0)
cat("The amount of words selected as predictive of a 5 star review is ", num_words_selected, "\n")
optimal_lambda <- lasso2_summary$lambda[optimal_index2]
beta_without_prodcat <- coef(lasso2)[-1:-143,]
1125 - sum(beta_without_prodcat == 0)
beta_without_prodcat_sort = sort(beta_without_prodcat, decreasing = TRUE)
beta_without_prodcat_sort[1:10]
exp(6.961539)
exp(beta_without_prodcat_sort[8])
```{r xtable data, results='asis'}
cv.fit <- cv.gamlr(x_cat2,
y=Y,
lambda.min.ratio=1e-3,
family="binomial",
verb=TRUE)
optimal_index3 <- which.min(cv.fit$lambda)
cv.fit$lambda
cv.fit$lambda.min
optimal_lambda <- cv.fit$lambda.min
optimal_coef2 <- coef(cv.fit, select = "min")
sum(optimal_coef2 != 0)
optimal_lambda <- cv.fit$lambda.min
optimal_coef2 <- coef(cv.fit, select = "min")
sum(optimal_coef2 != 0)
optimal_coef3 <- coef(cv.fit, select = "1se")
sum(optimal_coef3 != 0)
optimal_lambda <- cv.fit$lambda.min
optimal_coef2 <- coef(cv.fit)
sum(optimal_coef2 != 0)
optimal_coef3 <- coef(cv.fit, select = "1se")
sum(optimal_coef3 != 0)
optimal_lambda <- cv.fit$lambda.min
optimal_coef2 <- coef(cv.fit, select = "min")
sum(optimal_coef2 != 0)
optimal_coef3 <- coef(cv.fit, select = "1se")
sum(optimal_coef3 != 0)
